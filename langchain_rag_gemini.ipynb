{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install pypdf\n",
        "!pip install google-generativeai\n",
        "!pip install chromadb\n",
        "!pip install typing"
      ],
      "metadata": {
        "id": "VEqkdyFYnahp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from pypdf import PdfReader\n",
        "import os\n",
        "import re\n",
        "import google.generativeai as genai\n",
        "from chromadb import Documents, EmbeddingFunction, Embeddings\n",
        "import chromadb\n",
        "from typing import List\n",
        "\n",
        "# Download the PDF from the specified URL and save it to the given path\n",
        "def download_pdf(url, save_path):\n",
        "    response = requests.get(url)\n",
        "    with open(save_path, 'wb') as f:\n",
        "        f.write(response.content)\n",
        "\n",
        "# URL and local path for the PDF\n",
        "pdf_url = \"https://services.google.com/fh/files/misc/ai_adoption_framework_whitepaper.pdf\"\n",
        "pdf_path = \"ai_adoption_framework_whitepaper.pdf\"  # You can change this to a specific path if needed\n",
        "download_pdf(pdf_url, pdf_path)"
      ],
      "metadata": {
        "id": "XRl4q9weneyj"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the PDF file and extract text from each page\n",
        "def load_pdf(file_path):\n",
        "    reader = PdfReader(file_path)\n",
        "    text = \"\"\n",
        "    for page in reader.pages:\n",
        "        page_text = page.extract_text()\n",
        "        if page_text:\n",
        "            text += page_text\n",
        "    return text\n",
        "\n",
        "pdf_text = load_pdf(pdf_path)"
      ],
      "metadata": {
        "id": "taBOwhymnzri"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set and validate the API key for Gemini API\n",
        "from google.colab import userdata\n",
        "gemini_api_key = userdata.get('GOOGLE_API_KEY')\n",
        "if not gemini_api_key:\n",
        "    raise ValueError(\"Gemini API Key not provided or incorrect. Please provide a valid GEMINI_API_KEY.\")\n",
        "try:\n",
        "    genai.configure(api_key=gemini_api_key)\n",
        "    print(\"API configured successfully with the provided key.\")\n",
        "except Exception as e:\n",
        "    print(\"Failed to configure API:\", str(e))\n",
        "\n",
        "# Split the text into chunks based on double newlines\n",
        "def split_text(text):\n",
        "    return [i for i in re.split('\\n\\n', text) if i.strip()]\n",
        "\n",
        "chunked_text = split_text(pdf_text)\n"
      ],
      "metadata": {
        "id": "n3SN_P7Ln9wu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a custom embedding function using Gemini API\n",
        "class GeminiEmbeddingFunction(EmbeddingFunction):\n",
        "    def __call__(self, input: Documents) -> Embeddings:\n",
        "        gemini_api_key = userdata.get('GOOGLE_API_KEY')\n",
        "        genai.configure(api_key=gemini_api_key)\n",
        "        model = \"models/embedding-001\"\n",
        "        title = \"Custom query\"\n",
        "        return genai.embed_content(model=model, content=input, task_type=\"retrieval_document\", title=title)[\"embedding\"]\n"
      ],
      "metadata": {
        "id": "YWXQCcqEoRsZ"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create directory for database if it doesn't exist\n",
        "db_folder = \"chroma_db\"\n",
        "if not os.path.exists(db_folder):\n",
        "    os.makedirs(db_folder)\n",
        "\n",
        "# Create a Chroma database with the given documents\n",
        "def create_chroma_db(documents: List[str], path: str, name: str):\n",
        "    chroma_client = chromadb.PersistentClient(path=path)\n",
        "    db = chroma_client.create_collection(name=name, embedding_function=GeminiEmbeddingFunction())\n",
        "    for i, d in enumerate(documents):\n",
        "        db.add(documents=[d], ids=[str(i)])\n",
        "    return db, name\n",
        "\n",
        "# Specify the path and collection name for Chroma database\n",
        "db_name = \"rag_experiment\"\n",
        "db_path = os.path.join(os.getcwd(), db_folder)\n",
        "db, db_name = create_chroma_db(chunked_text, db_path, db_name)\n",
        "\n",
        "# Load an existing Chroma collection\n",
        "def load_chroma_collection(path: str, name: str):\n",
        "    chroma_client = chromadb.PersistentClient(path=path)\n",
        "    return chroma_client.get_collection(name=name, embedding_function=GeminiEmbeddingFunction())\n",
        "\n",
        "db = load_chroma_collection(db_path, db_name)\n"
      ],
      "metadata": {
        "id": "id-I9VQNof7W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Retrieve the most relevant passages based on the query\n",
        "def get_relevant_passage(query: str, db, n_results: int):\n",
        "    results = db.query(query_texts=[query], n_results=n_results)\n",
        "    return [doc[0] for doc in results['documents']]\n",
        "\n",
        "query = \"What is the AI Maturity Scale?\"\n",
        "relevant_text = get_relevant_passage(query, db, n_results=1)\n",
        "\n",
        "# Construct a prompt for the generation model based on the query and retrieved data\n",
        "def make_rag_prompt(query: str, relevant_passage: str):\n",
        "    escaped_passage = relevant_passage.replace(\"'\", \"\").replace('\"', \"\").replace(\"\\n\", \" \")\n",
        "    prompt = f\"\"\"You are a helpful and informative bot that answers questions using text from the reference passage included below.\n",
        "Be sure to respond in a complete sentence, being comprehensive, including all relevant background information.\n",
        "However, you are talking to a non-technical audience, so be sure to break down complicated concepts and\n",
        "strike a friendly and conversational tone.\n",
        "QUESTION: '{query}'\n",
        "PASSAGE: '{escaped_passage}'\n",
        "\n",
        "ANSWER:\n",
        "\"\"\"\n",
        "    return prompt\n"
      ],
      "metadata": {
        "id": "NZNZuy3konci"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate an answer using the Gemini Pro API\n",
        "def generate_answer(prompt: str):\n",
        "    gemini_api_key = userdata.get('GOOGLE_API_KEY')\n",
        "    if not gemini_api_key:\n",
        "        raise ValueError(\"Gemini API Key not provided. Please provide GEMINI_API_KEY as an environment variable\")\n",
        "    genai.configure(api_key=gemini_api_key)\n",
        "    model = genai.GenerativeModel('gemini-1.5-flash')\n",
        "    result = model.generate_content(prompt)\n",
        "    return result.text\n",
        "\n",
        "# Construct the prompt and generate the answer\n",
        "final_prompt = make_rag_prompt(query, \"\".join(relevant_text))\n",
        "answer = generate_answer(final_prompt)\n",
        "print(answer)"
      ],
      "metadata": {
        "id": "EX_PSfWkosk9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Interactive function to process user input and generate an answer\n",
        "def process_query_and_generate_answer():\n",
        "    query = input(\"Please enter your query: \")\n",
        "    if not query:\n",
        "        print(\"No query provided.\")\n",
        "        return\n",
        "    db = load_chroma_collection(db_path, db_name)\n",
        "    relevant_text = get_relevant_passage(query, db, n_results=1)\n",
        "    if not relevant_text:\n",
        "        print(\"No relevant information found for the given query.\")\n",
        "        return\n",
        "    final_prompt = make_rag_prompt(query, \"\".join(relevant_text))\n",
        "    answer = generate_answer(final_prompt)\n",
        "    print(\"Generated Answer:\", answer)\n",
        "\n",
        "# Invoke the function to interact with user\n",
        "process_query_and_generate_answer()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "id": "-zS-sEKWo_gS",
        "outputId": "4867fa6f-8a07-4345-f7c9-15b89b9c343a"
      },
      "execution_count": 9,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Please enter your query: how is ai revolutionizing the world\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-5-2204255465.py:22: DeprecationWarning: The class GeminiEmbeddingFunction does not implement __init__. This will be required in a future version.\n",
            "  return chroma_client.get_collection(name=name, embedding_function=GeminiEmbeddingFunction())\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Answer: AI is revolutionizing the world by enabling companies to improve, scale, and accelerate their decision-making processes.  This leads to increased efficiency and effectiveness across various business functions, opening up new avenues for revenue and providing a competitive edge.  In essence, companies investing in AI solutions are positioning themselves for future global economic leadership, potentially doubling their cash flow by 2030.  AI's power lies in its ability to find patterns in complex datasets and solve complex problems, impacting everything from predictive maintenance in manufacturing to personalized customer experiences in retail.  Recent technological advances, coupled with reduced costs in data storage and computing power, have made AI more accessible and versatile than ever before.\n",
            "\n"
          ]
        }
      ]
    }
  ]
}